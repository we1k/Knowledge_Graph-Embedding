Recently, Image-text matching is becoming a fast growing research field with the rapid development of both computer vision and natural language process. The image-text task seeks to find an embedding space where cross-modal embedding distances are correlated with the semantic similarity of original data. In this paper, we investigate some existing metrics for image-text matching and analyze their strengths and weaknesses in learning a shared embedding space for cross-modal data. We propose a novel objective function called cross-modal embedding distribution aware metric (CEDA), which significantly exploits the correlation of cross-modal data and efficiently utilizes the distribution of embeddings to enhance learning the shared space. Specifically, it makes the cross-modal data of the same semantic cluster compactly by making the center of the textual embeddings close to the corresponding visual embedding. Besides, instead of selecting a few pairs, it makes full use of the data in a batch to greatly exploit the embedding distribution and results in faster convergence. With a better learned space, the performance of image-text matching/retrieval can then be improved. We conduct our experiment on Flickr30K and MS-COCO with various backbone networks, and performances of chosen networks are greatly enhanced by using our proposed metric as objective.